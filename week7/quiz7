Grade:




_______________________________________________________________________________
Q1:

Suppose we have a linear feedforward network with two input nodes and one output node. Let's say that we are learning our weight vector w and that we are using the Hebb rule.

Suppose the input correlation matrix Q is:

Q=[0.2, 0.1; 0.1, 0.3]
If we allow learning to go on for a long period of time, which of these could be a final weight vector w?


A1:
 [-1.51 ; -1.30]

X

A1: [-1.57 ; -1.23]

X
A1: [0.8944 ; 1.7013]

(i looked at the forums and everyone is getting this question wrong...something something data)
X

A1: last answer
X
lol....so none of the answers were correct. Yup. I was getting different eigenvectors.
That explains it

_______________________________________________________________________________
Q2: What does the weight vector we found in Question 1 tell us?

A2: LTD
X
A2: none of these options



_______________________________________________________________________________
Q3: The "learning rate" as mentioned in lecture 7.2 is used in many different contexts to denote the sensitivity of a parameter estimate to new data during online learning. Check all of the following which are true:

A3:

A higher learning rate makes the parameter estimate more influenced by noise during online estimation.

A constant learning rate allows the parameter estimate to remain sensitive to new data.

_______________________________________________________________________________
Q4:
In the lectures, we saw multiple algorithms which use a two-step process for parameter estimation. EM is one such algorithm, consisting of the E and M steps. In each step, we iteratively update our estimates of parameters. Why do we need to alternate between the two steps? What justifies this approach?

(Hint: You may want to search "EM algorithm" on the web)

A4: We need the alternating steps because the two sets of parameters are mutually dependent on each other, necessitating that they be optimized together. This is justified by the fact that these algorithms will always attempt to move towards a better solution with each iteration.



_______________________________________________________________________________
Q5:

In the next five questions, we'll implement Oja's Hebb rule for a single neuron and explore some of its properties.

Recall that Oja's rule is:

τwdwdt=vu−αv2w,
where v=u⋅w.
We will implement the discrete time version of Oja's rule in Matlab or Octave. To do this, first rewrite Oja's rule using discrete time rather than continuous time. Which of the following equations represents the discrete time version of Oja's rule? (Let η=1τw).

A5: Δw=Δtη(vu−αv2w)



_______________________________________________________________________________
Q6:
Continued from Question 5.

Now, in order to use Oja's rule, we need to translate the discrete time version into an update rule. Which of the following equations represents the discrete update equation for Oja's rule?

A6: wi+1=wi+Δtη(vu−αv2w)



_______________________________________________________________________________
Q7: Continued from Question 5.

In this question, we will use the update rule we just derived to implement a neuron that will learn from two dimensional data that is given in the following Matlab file:

Assume our neuron receives as input the two dimensional data provided in c10p1.mat, but with the mean of the data subtracted from each data point (the mean of all x values should be subtracted from every x value and the mean of all y values should be subtracted from every y value). You should perform this zero-mean centering step and then display the points again to verify that the data cloud is now centered around (0,0).

Implement the update rule derived in the previous question in Matlab or Octave. Let η=1, α=1, and Δt=0.01. Start with a random vector as w0. In each update iteration, feed in a data point u=(x,y) from c10p1. If you've reached the last data point in c10p1, go back to the first one and repeat.

Typically, you would keep updating w until the change in w, given by norm(w(t+1) - w(t)), is negligibile (i.e., below an arbitrary small positive threshold), indicating that w has converged. However, since you are implementing this as an online learning algorithm, you may prematurely detect convergence using this method. Instead, you may just run the algorithm for 100,000 iterations.

Run your code multiple times. Which of the following describes the behavior of w and why does this happen?

Hint: Consider the eigenvectors of the correlation matrix of the mean-centered data. (The correlation matrix of a data matrix X, where rows indicate separate samples, is XTX/N, where N is the number of samples. You can calculate its eigenvalues using eig().) If the data is mean-centered, the correlation matrix will be the same as the covariance matrix.

A7: the values keep going back to 0.2...oscillating around

w always converges to the same unique vector because the correlation matrix of the input data has only one principal eigenvector.

X
A7: The input data is two dimensional and therefore the correlation matrix has two eigenvectors. w can converge to either of these eigenvectors regardless of their corresponding eigenvalues.

X
A7:
There is only one principal eigenvector of the correlation matrix, but there are two vectors of length 1α‾‾√ that are parallel to this eigenvector. w converges to either of these two vectors.

_______________________________________________________________________________
Q8: What happens when the data is not zero-mean centered before the learning process?

In order to more fully explore the behavior of the Oja's rule when the data isn't mean centered, you should adjust the mean of the data a few times and observe the behavior of the learning rule. You can adjust the mean of the data by adding a constant to every x component of the data and a different constant to every y component of the data.

A8: The two vectors that w converges to in different runs of the algorithm have the same direction as those found when the data is mean centered.

X
A8: The two vectors that w converges to in different runs of the algorithm are parallel to the vector that points roughly towards the mean of the data.

_______________________________________________________________________________
Q9: What happens when the pure Hebb rule is used instead of Oja's rule? You can explore what happens by removing the subtractive term −αv2w in your code and running the code.

A9: The vectors found by the learning rule have the same direction as those found by Oja's rule, but the length grows without bound as a function of the number of iterations.
